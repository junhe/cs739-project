% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}
\graphicspath{{./figures/}}
%\hyphenation{op-tical net-works semi-conduc-tor pre-fe-tch
%             log-struc-tured}

\usepackage{listings}
\lstset{language=C} 
             
\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\maketitle}{\@copyrightspace}{}{}{}
\makeatother



\begin{document}

\title{Creating Patterns by Data Shuffling \\for Distributed I/O Acceleration
\titlenote{This is a project report of course CS739 Distributed System
at University of Wisconsin, Madison.}
}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
\alignauthor
Jun He, Jia Xu\\
       \affaddr{Department of Computer Sciences}\\
       \affaddr{University of Wisconsin, Madison}\\
       \email{\{jhe, jia\}@cs.wisc.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Storage performance is critical for high
performance computer systems. PLFS (Parallel
Log-structured File System) has been developed
and it is able to speed up storage systems by
several orders of magnitudes. However, PLFS
is limited by its log, which grow to a
large size as the number of writes increase.
We propose to shuffle data among distributed
processes to make data writes more regular,
in order to enable PLFS's log compression
mechanism. The results show that we can reduce
the size of log significantly. In addition,
we try to reduce the data movement overhead
by careful scheduling.
\end{abstract}

%\keywords{Block allocation, ext4, file system} % NOT required for Proceedings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
High performance computing is critical to scientific
discoveries. It is widely used for weather forecasting,
DNA decoding, Physics simulation, financial data processing
and so on. Usually, high performance computer systems are
equipped with high-speed and low-latency
interconnection. The compute nodes frequently exchange
data with each other. Data on one node usually has
dependency with data on another node. 

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 110mm 150mm 0mm, clip, width=80mm]
    	{arch}
    \caption{An architecture of High Performance Computing system}
    \label{fig:arch}
\end{figure}

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 110mm 150mm 0mm, clip, width=80mm]
    	{layers}
    \caption{Software stack with PLFS}
    \label{fig:layers}
\end{figure}

Storage system in HPC system faces great challenges.
Figure~\ref{fig:arch} shows an architecture of
HPC system. Thousands of compute nodes are connected
with each other by the high-speed network. A storage
cluster is also attached to the network. There is
a cluster file system on the storage system. The popular
choices of cluster file system are GPFS from IBM,
Lustre from Intel (recently acquired by Intel),
PanFS from Panasas, and PVFS from Argonne 
National Laboratory. Applications like large-scale
simulations generate a great amount of data. The storage
system needs to provide TBs/sec or even PBs/sec
bandwidth to meet the requirements of the applications.
Using the storage system efficiently is hard. The
storage system performance highly depends the way the
application writes the data.

Parallel Log-structure File System has been developed
at Los Alamos National Lab to accelerate HPC storage
systems. The software stack is shown in Figure~\ref{fig:layers}
PLFS is presented as a library. Currently, there
are three ways of using PLFS. First, the application
can directly use PLFS calls, such as plfs\_open(),
plfs\_write(), to open or write a file. 
Second, the application can use a MPI (Message Passing
Interface) library that has been linked with PLFS. 
Third, the PLFS can be mounted as a FUSE 
\footnote{FUSE stands for file system in user space. fuse.sourceforge.net}
file system.
PLFS sits on top of a 'real' cluster file system.
Or we can consider PLFS as a middleware between
the application and the real cluster file system.
PLFS reorganizes the data accesses before the
data reaches the cluster file system. 
Results show that PLFS is able to speed up
the storage system by up to 150 times. 

However, PLFS has limitations. Since PLFS reorganizes
data, it needs a way to map the application's view
of the file and the physical view of the file. 
The log in PLFS serves this purpose. The log
records the information of each write the applicaion
conducts. As a result, the size of the log
is proportional to the number of writes the
application conducts. This leads to the problem
that the large log cannot be used efficiently
or it may even crash the file system in extreme
cases. Recently, a new feature has been proposed
to PLFS so that it can compress the log if the 
writes have regular patterns. This works
for most of the applications. But there are
some applications that do not have regular 
write patterns or occasionally lose patterns,
in which case the pattern compression cannot
take effect. 

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 110mm 150mm 0mm, clip, width=80mm]
    	{layers-shuffle}
    \caption{Software stack with data shuffling}
    \label{fig:layers-shuffle}
\end{figure}

In order to keep a small log for PLFS even if
there is no regular write patterns, we propose
to create patterns by shuffling data.
Figure~\ref{fig:layers-shuffle} shows where
the data shuffling component resides. It resides
inside the PLFS library but above other
PLFS components. Using the data shuffling
layer, we move data from one process to another
process. The later process will write the data
on behalf of the original process. The
data movements are transparent to the rest of
PLFS. The rest of the PLFS will treat the writes
as it was coming from the application directly.
If we shuffle data in a way that the resulting
writes form a pattern, PLFS can compress the log.

Shuffling data to create regular write pattern
leads to additional data movements across
the network. Scheduling of data movements 
is essential for
the proposed idea to work efficiently.
The order for each process to send/receive
data needs to be carefully calculated
so that we can have as many concurrent
data movements between processes as possible.
In this project, we propose an approach 
to schedule the data movement activities
to reduce data movement time.

This report is organized as follows.
Section~\ref{sec:related-work} introduces
related work. We describes the background
of PLFS in section~\ref{sec:plfs-background}.
In section~\ref{sec:creating-pattern},
we describe how to create pattern by data
shuffling.
In section~\ref{sec:scheduling-data-movements}
we describe how to carefully schedule data
movements to reduce data shuffling overhead.
The implementation details are in section~\ref{sec:implementation}.
Future work is in section~\ref{sec:future-work}.
Section~\ref{sec:conclusions} concludes the paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{sec:related-work}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PLFS background}
\label{sec:plfs-background}
In HPC environment, many processes run concurrently.
There are several ways for them to write data to
file. One is called \emph{N-1}. It means N processes
write to one file concurrently.
Another is called \emph{N-N}, which means N processes
write a N files. 

Generally, N-1 write is slower than N-N write. 
That is because, in file systems with locks, with
N-1 writes, when
one process is writing a segment of data, other
processes cannot write it. This serializes the
write to that segment. With N-N write, this is 
not a problem because each process writes
its own file. Nobody contends for data. 
Due to reasons like this, N-1 write can
be orders of magnitude slower than N-N write.
Although N-1 write is slower than N-N, users
like to use N-1 write. Having only one file
is easier for their data management. 

\begin{figure}[ht]
    \includegraphics
    	[trim=0mm 0mm 0mm 0mm, clip, width=80mm]
    	{plfs-logical-physical}
    \caption{PLFS transparently transfers N-1 write to N-N write.}
    \label{fig:plfs-logical-physical}
\end{figure}

PLFS has been developed to mitigate this issue
by transparently transferring N-1 writes to N-N
write. Figure~\ref{fig:plfs-logical-physical}
shows how it works. From the application's
perspective, the data is the same as it were
without PLFS. The PLFS provides this illusion
to application so the application can work 
without modification. However, internally,
PLFS reorganizes data so that there are
one physical file for each process. For example,
if there are 6 processes writing to one file
called \emph{fileshared}. The application
sees only one file in PLFS. But PLFS actually
stores 6 data files in the underlying
cluster file system. 

Since the data is reorganized. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Creating pattern}
\label{sec:creating-pattern}
\subsection{The pattern detection optimization}
local pattern

global pattern

\subsection{Problem of irregular writes}
Show LANL App2 graph as an example.

\subsection{Shuffling data to create patterns}
How it works: move data to another process and let
that process to write on behalf of the original writer.

\subsection{Discussion}
What's a good patterns to make among 
all possible patterns?

How can we choose a pattern to minimize
amount of data to be shuffled?

How can we decide if it is worth it to
shuffle data? - the cost of shuffling might be
too high.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scheduling data movements}
\label{sec:scheduling-data-movements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\label{sec:implementation}

Framework: 
the replayer, 

the master, 

the pattern decider: the data structures

the scheduler: data structures


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
\label{sec:evaluation}
Test setup

Test log size compression

Shuffling bandwidth


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}
\label{sec:future-work}
Better pattern

Better scheduling


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}





\bibliographystyle{abbrv}
\bibliography{sigproc}  
\end{document}
